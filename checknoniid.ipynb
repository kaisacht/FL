{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from CIFAR10 dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "\n",
    "def cifar_noniid(dataset_label, num_clients, num_classes, q):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from CIFAR10 dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    proportion = non_iid_distribution_group(dataset_label, num_clients, num_classes, q)\n",
    "    dict_users = non_iid_distribution_client(proportion, num_clients, num_classes)\n",
    "    #  output clients' labels information\n",
    "    check_data_each_client(dataset_label, dict_users, num_clients, num_classes)\n",
    "    return dict_users\n",
    "\n",
    "def non_iid_distribution_group(dataset_label, num_clients, num_classes, q):\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset_label))]\n",
    "    for i in range(num_classes):\n",
    "        dict_users[i] = set([])\n",
    "    for k in range(num_classes):\n",
    "        idx_k = np.where(dataset_label == k)[0]\n",
    "        num_idx_k = len(idx_k)\n",
    "        \n",
    "        selected_q_data = set(np.random.choice(idx_k, int(num_idx_k*q) , replace=False))\n",
    "        dict_users[k] = dict_users[k]|selected_q_data\n",
    "        idx_k = list(set(idx_k) - selected_q_data)\n",
    "        all_idxs = list(set(all_idxs) - selected_q_data)\n",
    "        for other_group in range(num_classes):\n",
    "            if other_group == k:\n",
    "                continue\n",
    "            selected_not_q_data = set(np.random.choice(idx_k, int(num_idx_k*(1-q)/(num_classes-1)) , replace=False))\n",
    "            dict_users[other_group] = dict_users[other_group]|selected_not_q_data\n",
    "            idx_k = list(set(idx_k) - selected_not_q_data)\n",
    "            all_idxs = list(set(all_idxs) - selected_not_q_data)\n",
    "    print(len(all_idxs),' samples are remained')\n",
    "    print('random put those samples into groups')\n",
    "    num_rem_each_group = len(all_idxs) // num_classes\n",
    "    for i in range(num_classes):\n",
    "        selected_rem_data = set(np.random.choice(all_idxs, num_rem_each_group, replace=False))\n",
    "        dict_users[i] = dict_users[i]|selected_rem_data\n",
    "        all_idxs = list(set(all_idxs) - selected_rem_data)\n",
    "    print(len(all_idxs),' samples are remained after relocating')\n",
    "    return dict_users\n",
    "\n",
    "def non_iid_distribution_client(group_proportion, num_clients, num_classes):\n",
    "    num_each_group = num_clients // num_classes\n",
    "    num_data_each_client = len(group_proportion[0]) // num_each_group\n",
    "    dict_users, all_idxs = {}, [i for i in range(num_data_each_client*num_clients)]\n",
    "    for i in range(num_classes):\n",
    "        group_data = list(group_proportion[i])\n",
    "        for j in range(num_each_group):\n",
    "            selected_data = set(np.random.choice(group_data, num_data_each_client, replace=False))\n",
    "            dict_users[i*10+j] = selected_data\n",
    "            group_data = list(set(group_data) - selected_data)\n",
    "            all_idxs = list(set(all_idxs) - selected_data)\n",
    "    print(len(all_idxs),' samples are remained')\n",
    "    return dict_users\n",
    "def check_data_each_client(dataset_label, client_data_proportion, num_client, num_classes):\n",
    "    for client in client_data_proportion.keys():\n",
    "        client_data = dataset_label[list(client_data_proportion[client])]\n",
    "        print('client', client, 'distribution information:')\n",
    "        for i in range(num_classes):\n",
    "            print('class ', i, ':', len(client_data[client_data==i])/len(client_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mnist_noniid(dataset, num_users):\n",
    "#     \"\"\"\n",
    "#     Sample non-I.I.D client data from MNIST dataset\n",
    "#     :param dataset:\n",
    "#     :param num_users:\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     num_shards, num_imgs = 200, 300\n",
    "#     idx_shard = [i for i in range(num_shards)]\n",
    "#     dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "#     idxs = np.arange(num_shards*num_imgs)\n",
    "#     labels = dataset.train_labels.numpy()\n",
    "\n",
    "#     # sort labels\n",
    "#     idxs_labels = np.vstack((idxs, labels))\n",
    "#     idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n",
    "#     idxs = idxs_labels[0,:]\n",
    "\n",
    "#     # divide and assign\n",
    "#     for i in range(num_users):\n",
    "#         rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "#         idx_shard = list(set(idx_shard) - rand_set)\n",
    "#         for rand in rand_set:\n",
    "#             dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    # return dict_users\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def mnist_custom_non_iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample Non-I.I.D. client data from MNIST dataset based on specified label distribution\n",
    "    :param dataset: MNIST dataset\n",
    "    :param num_users: number of users\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    dict_users = defaultdict(set)\n",
    "\n",
    "    # Get indices for each class\n",
    "    idx_by_class = {label: np.where(dataset.targets == label)[0] for label in range(10)}\n",
    "\n",
    "    # Assign labels to users\n",
    "    labels_for_users = {\n",
    "        0: 250,  # 1 class with 250 labels\n",
    "        1: np.random.randint(20, 26),  # Random number of labels for 9 classes (20 to 25)\n",
    "        2: np.random.randint(20, 26),\n",
    "        3: np.random.randint(20, 26),\n",
    "        4: np.random.randint(20, 26),\n",
    "        5: np.random.randint(20, 26),\n",
    "        6: np.random.randint(20, 26),\n",
    "        7: np.random.randint(20, 26),\n",
    "        8: np.random.randint(20, 26),\n",
    "        9: np.random.randint(20, 26),\n",
    "    }\n",
    "\n",
    "    # Assign data to each user based on specified label distribution\n",
    "    for i in range(num_users):\n",
    "        for label, num_samples in labels_for_users.items():\n",
    "            sampled_indices = np.random.choice(idx_by_class[label], size=num_samples, replace=False)\n",
    "            dict_users[i] = dict_users[i].union(set(sampled_indices))\n",
    "\n",
    "    return dict_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                   ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100\n",
    "d = mnist_custom_non_iid(dataset_train, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_noniid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_shards, num_imgs = 400, 150\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = dataset.train_labels.numpy()\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n",
    "    idxs = idxs_labels[0,:]\n",
    "\n",
    "    # divide and assign\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 4, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100\n",
    "d = mnist_noniid(dataset_train, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_users = np.load('./data/non_iid_cifar.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client:  0 [0, 300, 0, 0, 0, 150, 0, 150, 0, 0]\n",
      "client:  1 [150, 0, 0, 0, 150, 0, 150, 150, 0, 0]\n",
      "client:  2 [150, 0, 0, 0, 0, 150, 150, 150, 0, 0]\n",
      "client:  3 [0, 0, 0, 150, 0, 150, 150, 0, 150, 0]\n",
      "client:  4 [0, 0, 150, 0, 0, 150, 0, 0, 150, 150]\n",
      "client:  5 [0, 0, 150, 0, 300, 0, 0, 150, 0, 0]\n",
      "client:  6 [150, 0, 0, 0, 0, 150, 0, 150, 150, 0]\n",
      "client:  7 [0, 0, 0, 150, 300, 0, 0, 0, 0, 150]\n",
      "client:  8 [0, 300, 0, 150, 0, 0, 0, 0, 0, 150]\n",
      "client:  9 [0, 150, 0, 150, 0, 0, 0, 150, 0, 150]\n",
      "client:  10 [0, 0, 150, 150, 0, 0, 0, 150, 0, 150]\n",
      "client:  11 [150, 150, 150, 0, 0, 0, 150, 0, 0, 0]\n",
      "client:  12 [0, 150, 0, 0, 0, 0, 0, 300, 150, 0]\n",
      "client:  13 [0, 0, 300, 0, 150, 0, 0, 150, 0, 0]\n",
      "client:  14 [150, 150, 150, 0, 150, 0, 0, 0, 0, 0]\n",
      "client:  15 [150, 0, 0, 150, 0, 0, 150, 0, 150, 0]\n",
      "client:  16 [0, 150, 150, 0, 0, 0, 0, 0, 0, 300]\n",
      "client:  17 [0, 150, 150, 0, 0, 0, 150, 0, 150, 0]\n",
      "client:  18 [300, 0, 0, 300, 0, 0, 0, 0, 0, 0]\n",
      "client:  19 [0, 0, 0, 0, 150, 0, 150, 0, 0, 300]\n",
      "client:  20 [150, 150, 150, 0, 0, 0, 150, 0, 0, 0]\n",
      "client:  21 [0, 150, 150, 150, 0, 0, 150, 0, 0, 0]\n",
      "client:  22 [0, 0, 150, 0, 0, 150, 300, 0, 0, 0]\n",
      "client:  23 [0, 0, 0, 0, 0, 0, 0, 450, 0, 150]\n",
      "client:  24 [0, 0, 0, 0, 150, 150, 300, 0, 0, 0]\n",
      "client:  25 [0, 0, 150, 150, 0, 0, 0, 0, 201, 99]\n",
      "client:  26 [0, 0, 0, 0, 150, 150, 150, 150, 0, 0]\n",
      "client:  27 [150, 0, 0, 150, 150, 150, 0, 0, 0, 0]\n",
      "client:  28 [150, 0, 0, 0, 0, 150, 0, 150, 0, 150]\n",
      "client:  29 [150, 150, 0, 150, 0, 0, 0, 0, 0, 150]\n",
      "client:  30 [0, 150, 150, 150, 0, 0, 0, 0, 0, 150]\n",
      "client:  31 [150, 0, 0, 150, 0, 0, 150, 0, 0, 150]\n",
      "client:  32 [0, 0, 0, 150, 0, 150, 0, 150, 150, 0]\n",
      "client:  33 [0, 0, 0, 150, 0, 0, 0, 150, 300, 0]\n",
      "client:  34 [0, 150, 0, 150, 0, 150, 0, 0, 150, 0]\n",
      "client:  35 [0, 150, 0, 0, 0, 0, 0, 150, 0, 300]\n",
      "client:  36 [150, 0, 0, 0, 0, 150, 0, 0, 0, 300]\n",
      "client:  37 [0, 0, 0, 0, 0, 0, 0, 150, 300, 150]\n",
      "client:  38 [0, 0, 150, 0, 0, 0, 300, 150, 0, 0]\n",
      "client:  39 [0, 150, 150, 0, 0, 150, 0, 0, 150, 0]\n",
      "client:  40 [0, 0, 150, 150, 0, 150, 0, 150, 0, 0]\n",
      "client:  41 [300, 0, 0, 0, 0, 0, 150, 0, 150, 0]\n",
      "client:  42 [150, 0, 150, 0, 0, 150, 0, 150, 0, 0]\n",
      "client:  43 [150, 150, 0, 0, 0, 150, 0, 0, 0, 150]\n",
      "client:  44 [0, 0, 150, 150, 0, 0, 150, 0, 0, 150]\n",
      "client:  45 [300, 0, 23, 127, 0, 17, 133, 0, 0, 0]\n",
      "client:  46 [300, 0, 0, 150, 0, 0, 0, 0, 0, 150]\n",
      "client:  47 [0, 0, 0, 150, 0, 0, 300, 0, 0, 150]\n",
      "client:  48 [0, 0, 0, 150, 150, 300, 0, 0, 0, 0]\n",
      "client:  49 [0, 150, 0, 0, 150, 0, 0, 150, 150, 0]\n",
      "client:  50 [0, 0, 0, 150, 150, 0, 150, 0, 150, 0]\n",
      "client:  51 [0, 0, 300, 150, 150, 0, 0, 0, 0, 0]\n",
      "client:  52 [0, 150, 450, 0, 0, 0, 0, 0, 0, 0]\n",
      "client:  53 [0, 450, 0, 0, 0, 0, 0, 0, 0, 150]\n",
      "client:  54 [73, 227, 0, 0, 150, 0, 0, 0, 0, 150]\n",
      "client:  55 [0, 0, 150, 0, 150, 0, 150, 150, 0, 0]\n",
      "client:  56 [150, 0, 150, 0, 0, 0, 0, 50, 250, 0]\n",
      "client:  57 [0, 300, 0, 0, 0, 150, 0, 0, 0, 150]\n",
      "client:  58 [0, 0, 0, 150, 150, 0, 300, 0, 0, 0]\n",
      "client:  59 [0, 150, 0, 300, 0, 0, 150, 0, 0, 0]\n",
      "client:  60 [0, 150, 0, 150, 0, 150, 0, 150, 0, 0]\n",
      "client:  61 [0, 0, 450, 0, 0, 0, 0, 150, 0, 0]\n",
      "client:  62 [150, 300, 150, 0, 0, 0, 0, 0, 0, 0]\n",
      "client:  63 [0, 150, 0, 0, 150, 150, 150, 0, 0, 0]\n",
      "client:  64 [0, 150, 0, 0, 0, 0, 0, 150, 300, 0]\n",
      "client:  65 [150, 0, 0, 0, 0, 0, 0, 150, 300, 0]\n",
      "client:  66 [0, 0, 0, 0, 450, 0, 150, 0, 0, 0]\n",
      "client:  67 [150, 0, 150, 0, 0, 0, 0, 0, 150, 150]\n",
      "client:  68 [0, 300, 0, 150, 0, 0, 150, 0, 0, 0]\n",
      "client:  69 [150, 0, 0, 150, 150, 150, 0, 0, 0, 0]\n",
      "client:  70 [0, 150, 0, 0, 0, 0, 150, 0, 150, 150]\n",
      "client:  71 [0, 65, 85, 150, 0, 150, 0, 0, 150, 0]\n",
      "client:  72 [0, 0, 150, 0, 150, 150, 0, 0, 0, 150]\n",
      "client:  73 [0, 150, 0, 0, 150, 150, 0, 150, 0, 0]\n",
      "client:  74 [150, 150, 0, 0, 146, 4, 0, 150, 0, 0]\n",
      "client:  75 [150, 0, 0, 0, 300, 0, 0, 0, 0, 150]\n",
      "client:  76 [0, 300, 0, 150, 0, 0, 150, 0, 0, 0]\n",
      "client:  77 [0, 0, 0, 0, 150, 0, 0, 150, 150, 150]\n",
      "client:  78 [0, 0, 0, 0, 0, 0, 0, 300, 150, 150]\n",
      "client:  79 [0, 150, 150, 0, 0, 150, 0, 0, 150, 0]\n",
      "client:  80 [0, 150, 150, 0, 150, 0, 0, 0, 150, 0]\n",
      "client:  81 [150, 0, 150, 0, 0, 150, 150, 0, 0, 0]\n",
      "client:  82 [0, 0, 150, 0, 150, 150, 0, 0, 0, 150]\n",
      "client:  83 [150, 0, 0, 150, 150, 150, 0, 0, 0, 0]\n",
      "client:  84 [0, 0, 150, 0, 150, 300, 0, 0, 0, 0]\n",
      "client:  85 [0, 0, 0, 150, 0, 0, 0, 300, 0, 150]\n",
      "client:  86 [150, 0, 0, 150, 150, 0, 0, 0, 150, 0]\n",
      "client:  87 [150, 0, 0, 0, 0, 150, 0, 150, 150, 0]\n",
      "client:  88 [0, 0, 0, 154, 296, 0, 0, 0, 150, 0]\n",
      "client:  89 [0, 150, 0, 0, 0, 150, 0, 150, 0, 150]\n",
      "client:  90 [0, 150, 0, 0, 0, 0, 150, 150, 0, 150]\n",
      "client:  91 [0, 150, 0, 0, 150, 150, 0, 150, 0, 0]\n",
      "client:  92 [0, 0, 0, 150, 0, 0, 300, 150, 0, 0]\n",
      "client:  93 [150, 0, 0, 0, 0, 0, 0, 0, 300, 150]\n",
      "client:  94 [300, 0, 150, 0, 0, 0, 0, 0, 150, 0]\n",
      "client:  95 [150, 0, 0, 0, 0, 0, 150, 0, 150, 150]\n",
      "client:  96 [0, 0, 0, 300, 300, 0, 0, 0, 0, 0]\n",
      "client:  97 [150, 0, 0, 0, 0, 0, 300, 150, 0, 0]\n",
      "client:  98 [150, 0, 150, 150, 0, 150, 0, 0, 0, 0]\n",
      "client:  99 [0, 0, 0, 0, 0, 0, 85, 65, 300, 150]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    client  = [0,0,0,0,0,0,0,0,0,0]\n",
    "    for j in range(len(d[i])):\n",
    "        my_list = list(d[i])\n",
    "        idx = my_list[j]\n",
    "        label = dataset_train[idx][1]\n",
    "        client[label] += 1\n",
    "    print(\"client: \", i , client)\n",
    "    client = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
